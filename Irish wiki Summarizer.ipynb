{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пишем токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "punctuation = punctuation + '\\n'\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8500/4265195184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train([\"ga_dataset.txt\"], min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!mkdir gawikimodel\n",
    "tokenizer.save_model(\"gawikimodel\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./gawikimodel/vocab.json\",\n",
    "    \"./gawikimodel/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./gawikimodel\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"ga_dataset.txt\",\n",
    "    block_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gawikimodel\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=20,\n",
    "    prediction_loss_only=True,\n",
    "    no_cuda=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\envs\\wikienv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./gawikimodel\", max_len=512)\n",
    "model = RobertaForMaskedLM.from_pretrained('./gawikimodel/checkpoint-4000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tr(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r'http\\S+',' ',sentence)\n",
    "  sentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "  sentence = sentence.split()\n",
    "  sentence = [lem.lemmatize(word) for word in sentence]\n",
    "  sentence = ' '.join(sentence)\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "def textrank(text, num_sentences=3):\n",
    "    # Text into sentences\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Text into words\n",
    "    prepared_sentences = [clean_tr(sentence) for sentence in sentences]\n",
    "    words = [sentence.split() for sentence in prepared_sentences]\n",
    "    words = sum(words, []) #flatten the list\n",
    "    # calculate word frequencies\n",
    "    fdist = FreqDist(words)\n",
    "    \n",
    "    # Assign scores to sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(prepared_sentences):\n",
    "        for word in sentence.split():\n",
    "            if word in fdist:\n",
    "                if i in sentence_scores:\n",
    "                    sentence_scores[i] += fdist[word]\n",
    "                else:\n",
    "                    sentence_scores[i] = fdist[word]\n",
    "    \n",
    "    # Sort sentences by scores in descending order\n",
    "    sorted_sentences = sorted(sentence_scores, key=lambda x: sentence_scores[x], reverse=True)\n",
    "    \n",
    "    # Select the top `num_sentences` sentences for the summary\n",
    "    summary_sentences = sorted(sorted_sentences[:num_sentences])\n",
    "    \n",
    "    # Create the summary\n",
    "    summary = ' '.join([sentences[i] for i in summary_sentences])\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SumBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sb(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r'http\\S+',' ',sentence)\n",
    "  sentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "  sentence = sentence.split()\n",
    "  sentence = [lem.lemmatize(word) for word in sentence]\n",
    "  sentence = ' '.join(sentence)\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_probability(sentences):\n",
    "    probability_dict = {}\n",
    "    #words = '. '.join(sentences)\n",
    "    words = [sentence.split() for sentence in sentences]\n",
    "    words = sum(words, []) #flatten the list\n",
    "    total_words = len(set(words))\n",
    "    for word in words:\n",
    "        if word!='.':\n",
    "            if not probability_dict.get(word):\n",
    "                probability_dict[word] = 1\n",
    "            else:\n",
    "                probability_dict[word] += 1\n",
    "    \n",
    "    for word,count in probability_dict.items():\n",
    "        probability_dict[word] = count/total_words \n",
    "    \n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_probability(probability_dict,word):\n",
    "    if probability_dict.get(word):\n",
    "        try:\n",
    "            probability_dict[word] = probability_dict[word]**2\n",
    "        except:\n",
    "            pass\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_weights(sentences,probability_dict):\n",
    "\tsentence_weights = {}\n",
    "\tfor index,sentence in enumerate(sentences):\n",
    "\t\tif len(sentence) != 0:\n",
    "\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n",
    "\t\t\taverage_proba /= len(sentence)\n",
    "\t\t\tsentence_weights[index] = average_proba \n",
    "\treturn sentence_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n",
    "    summary = \"\"\n",
    "    current_length = 0\n",
    "    prev_sentence = []\n",
    "    while current_length < summary_length :\n",
    "        \n",
    "        highest_probability_word = max(probability_dict)\n",
    "        #print(highest_probability_word)\n",
    "        sentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in sentence.split(' ')]\n",
    "        sentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n",
    "        #while ((sentence_list[0][0]) not in prev_sentence):\n",
    "        summary += cleaned_article[sentence_list[0][0]] + \". \"\n",
    "            #prev_sentence.append(sentence_list[0][0])\n",
    "            #sentence_list[0].pop(0)\n",
    "        for word in cleaned_article[sentence_list[0][0]]:\n",
    "            probability_dict = update_probability(probability_dict,word)\n",
    "        current_length+=1\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumbasic(article, required_length):\n",
    "    cleaned_article = []\n",
    "    for i in article.split('.'):\n",
    "        cleaned_article.append(clean_sb(i))\n",
    "    tokenized_article = tokenizer.encode(article)\n",
    "    #cleaned_article = clean(tokenized_article)\n",
    "    probability_dict = init_probability(cleaned_article)\n",
    "    #print(probability_dict.get('b'))\n",
    "    sentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n",
    "    summary = generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,required_length)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# luhn sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lh(article):\n",
    "\tlem = WordNetLemmatizer()\n",
    "\tarticle =  re.sub(r'\\[[^\\]]*\\]','',article)\n",
    "\tarticle = article.split('.')\n",
    "\tcleaned_list=[]\n",
    "\tfor sent in article:\n",
    "\t\tsent  = sent.lower()\n",
    "\t\tword_list = []\n",
    "\t\twords = sent.split()\n",
    "\t\tfor word in words:\n",
    "\t\t\tword_list.append(lem.lemmatize(word.lower()))\n",
    "\t\tcleaned_list.append(' '.join(word_list))\n",
    "\treturn cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_dictionary(content):\n",
    "\tfrequency = {}\n",
    "\tfor sentence in content:\n",
    "\t\tword_list = sentence.split()#word_tokenize(sentence)\n",
    "\t\tfor word in word_list:\n",
    "\t\t\tif word not in [',','.',';','%',')','(','``']:\n",
    "\t\t\t\tif frequency.get(word) is None:\n",
    "\t\t\t\t\tfrequency[word]=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfrequency[word]+=1\n",
    "\treturn frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(content,frequency_dictionary):\n",
    "    sentence_score={}\n",
    "    for sentence in content:\n",
    "        score=0\n",
    "        word_list = sentence.split()\n",
    "        start_idx,end_idx = -1,len(word_list)+1\n",
    "        index_list=[]\n",
    "        for word in word_list:\n",
    "            if word not in [',','.',';','%',')','(','``'] and word in frequency_dictionary.keys():\n",
    "                index_list.append(word_list.index(word)+1)\n",
    "            if index_list:\n",
    "                if max(index_list) != min(index_list):\n",
    "                    score = len(index_list)**2/(max(index_list) - min(index_list))\n",
    "                else:\n",
    "                    score = len(index_list)**2/max(index_list)\n",
    "        sentence_score[content.index(sentence)] = score\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_luhn(sentence_scores,content,threshold):\n",
    "    summary = \"\"\n",
    "    sentence_indexes = sorted(sentence_scores,key=sentence_scores.get,reverse=True)\n",
    "    for index in sentence_indexes:\n",
    "        summary+=content[index]+\" \" \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn(content, word_limit):\n",
    "    cleaned_content = clean_lh(content)\n",
    "    threshold = len(cleaned_content)//40\n",
    "    frequency_dictionary = get_frequency_dictionary(cleaned_content)\n",
    "    sorted_dictionary = {key: frequency_dictionary[key] for key in sorted(frequency_dictionary,key=frequency_dictionary.get,reverse=True)[:word_limit]}\n",
    "    sentence_scores = get_score(cleaned_content,sorted_dictionary)\n",
    "    summary = get_summary_luhn(sentence_scores,cleaned_content,threshold)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_data = []\n",
    "with open('train.json', 'r', encoding='UTF-8') as file:\n",
    "    for line in file:\n",
    "        train_data.append(json.loads(line))\n",
    "original_list = [train_data[i]['reference'] for i in range(10)]\n",
    "test_summary_list = [train_data[i]['summary'] for i in range(10)]\n",
    "length_list = [len(i.split('.')) for i in test_summary_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "lem = WordNetLemmatizer()\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn in text 0\n",
      "{'rouge1': Score(precision=0.09705541669206852, recall=0.8943820224719101, fmeasure=0.17510861793983393), 'rougeL': Score(precision=0.047125525818447846, recall=0.43426966292134833, fmeasure=0.08502447340922839)}\n",
      "Sumbasic in text 0\n",
      "{'rouge1': Score(precision=0.10363636363636364, recall=0.22415730337078651, fmeasure=0.14174067495559503), 'rougeL': Score(precision=0.08025974025974025, recall=0.17359550561797754, fmeasure=0.10976909413854351)}\n",
      "Textrank in text 0\n",
      "{'rouge1': Score(precision=0.11187632793611253, recall=0.8578651685393258, fmeasure=0.19793894614038499), 'rougeL': Score(precision=0.05047988863653015, recall=0.38707865168539324, fmeasure=0.08931233391665046)}\n",
      "Luhn in text 1\n",
      "{'rouge1': Score(precision=0.040940465133220416, recall=0.914936914936915, fmeasure=0.07837394972631871), 'rougeL': Score(precision=0.02469540512484292, recall=0.5518925518925519, fmeasure=0.04727538960359794)}\n",
      "Sumbasic in text 1\n",
      "{'rouge1': Score(precision=0.1146384479717813, recall=0.291005291005291, fmeasure=0.16448125143777315), 'rougeL': Score(precision=0.08257174923841591, recall=0.2096052096052096, fmeasure=0.11847250977685762)}\n",
      "Textrank in text 1\n",
      "{'rouge1': Score(precision=0.12270769230769231, recall=0.8115588115588116, fmeasure=0.21318223125033411), 'rougeL': Score(precision=0.05803076923076923, recall=0.3838013838013838, fmeasure=0.10081787566151709)}\n",
      "Luhn in text 2\n",
      "{'rouge1': Score(precision=0.032560500930783554, recall=0.9092627599243857, fmeasure=0.06286965330196387), 'rougeL': Score(precision=0.019495684548993062, recall=0.5444234404536862, fmeasure=0.03764336829722576)}\n",
      "Sumbasic in text 2\n",
      "{'rouge1': Score(precision=0.07212974818608621, recall=0.31947069943289225, fmeasure=0.11768802228412256), 'rougeL': Score(precision=0.05335040546308152, recall=0.23629489603024575, fmeasure=0.08704735376044569)}\n",
      "Textrank in text 2\n",
      "{'rouge1': Score(precision=0.1025260029717682, recall=0.782608695652174, fmeasure=0.18130063499014673), 'rougeL': Score(precision=0.047795938583457154, recall=0.3648393194706994, fmeasure=0.08451937814758047)}\n",
      "Luhn in text 3\n",
      "{'rouge1': Score(precision=0.14825225225225225, recall=0.8974694589877836, fmeasure=0.25446898002103047), 'rougeL': Score(precision=0.060828828828828826, recall=0.3682373472949389, fmeasure=0.10441021834601348)}\n",
      "Sumbasic in text 3\n",
      "{'rouge1': Score(precision=0.059629421673217296, recall=0.46335078534031415, fmeasure=0.10566112824594569), 'rougeL': Score(precision=0.04385176866928692, recall=0.34075043630017454, fmeasure=0.07770371107352503)}\n",
      "Textrank in text 3\n",
      "{'rouge1': Score(precision=0.3244452397995705, recall=0.7910122164048866, fmeasure=0.4601522842639595), 'rougeL': Score(precision=0.12097351467430208, recall=0.2949389179755672, fmeasure=0.1715736040609137)}\n",
      "Luhn in text 4\n",
      "{'rouge1': Score(precision=0.5686274509803921, recall=0.5610997963340122, fmeasure=0.5648385443362377), 'rougeL': Score(precision=0.24871001031991744, recall=0.2454175152749491, fmeasure=0.24705279343926193)}\n",
      "Sumbasic in text 4\n",
      "{'rouge1': Score(precision=0.16161616161616163, recall=0.0814663951120163, fmeasure=0.1083276912660799), 'rougeL': Score(precision=0.13737373737373737, recall=0.06924643584521385, fmeasure=0.09207853757616792)}\n",
      "Textrank in text 4\n",
      "{'rouge1': Score(precision=0.5664263645726055, recall=0.560081466395112, fmeasure=0.5632360471070149), 'rougeL': Score(precision=0.28527291452111225, recall=0.2820773930753564, fmeasure=0.28366615463389655)}\n",
      "Luhn in text 5\n",
      "{'rouge1': Score(precision=0.39939485627836613, recall=0.6, fmeasure=0.4795640326975477), 'rougeL': Score(precision=0.170196671709531, recall=0.2556818181818182, fmeasure=0.20435967302452315)}\n",
      "Sumbasic in text 5\n",
      "{'rouge1': Score(precision=0.2375, recall=0.12954545454545455, fmeasure=0.1676470588235294), 'rougeL': Score(precision=0.17083333333333334, recall=0.09318181818181819, fmeasure=0.12058823529411763)}\n",
      "Textrank in text 5\n",
      "{'rouge1': Score(precision=0.39893617021276595, recall=0.5965909090909091, fmeasure=0.4781420765027322), 'rougeL': Score(precision=0.1595744680851064, recall=0.23863636363636365, fmeasure=0.19125683060109291)}\n",
      "Luhn in text 6\n",
      "{'rouge1': Score(precision=0.4265486725663717, recall=0.5239130434782608, fmeasure=0.47024390243902436), 'rougeL': Score(precision=0.1672566371681416, recall=0.20543478260869566, fmeasure=0.18439024390243902)}\n",
      "Sumbasic in text 6\n",
      "{'rouge1': Score(precision=0.03882086167800453, recall=0.4652173913043478, fmeasure=0.07166178317287566), 'rougeL': Score(precision=0.03346938775510204, recall=0.40108695652173915, fmeasure=0.06178317287568019)}\n",
      "Textrank in text 6\n",
      "{'rouge1': Score(precision=0.4276841171251109, recall=0.5239130434782608, fmeasure=0.47093307278944796), 'rougeL': Score(precision=0.16947648624667258, recall=0.2076086956521739, fmeasure=0.18661455788959452)}\n",
      "Luhn in text 7\n",
      "{'rouge1': Score(precision=0.10431112451009948, recall=0.9049694856146469, fmeasure=0.1870607316633628), 'rougeL': Score(precision=0.051050145713998595, recall=0.44289450741063646, fmeasure=0.09154802667147234)}\n",
      "Sumbasic in text 7\n",
      "{'rouge1': Score(precision=0.044096728307254626, recall=0.5945945945945946, fmeasure=0.08210437609101306), 'rougeL': Score(precision=0.034333376438639596, recall=0.4629468177855275, fmeasure=0.06392584120869199)}\n",
      "Textrank in text 7\n",
      "{'rouge1': Score(precision=0.1983454398708636, recall=0.8570183086312119, fmeasure=0.32213665410453873), 'rougeL': Score(precision=0.08797417271993543, recall=0.3801220575414124, fmeasure=0.14288055054891036)}\n",
      "Luhn in text 8\n",
      "{'rouge1': Score(precision=0.09654969613801215, recall=0.8979033728350045, fmeasure=0.17435171254093282), 'rougeL': Score(precision=0.04646147814154088, recall=0.43208751139471285, fmeasure=0.08390123019736259)}\n",
      "Sumbasic in text 8\n",
      "{'rouge1': Score(precision=0.1583011583011583, recall=0.18687329079307202, fmeasure=0.1714046822742475), 'rougeL': Score(precision=0.12123552123552124, recall=0.1431175934366454, fmeasure=0.13127090301003344)}\n",
      "Textrank in text 8\n",
      "{'rouge1': Score(precision=0.2541776605101143, recall=0.7903372835004557, fmeasure=0.38464951197870445), 'rougeL': Score(precision=0.09469363822925828, recall=0.2944393801276208, fmeasure=0.14330079858030167)}\n",
      "Luhn in text 9\n",
      "{'rouge1': Score(precision=0.05595963567262958, recall=0.8148854961832062, fmeasure=0.10472745110061928), 'rougeL': Score(precision=0.025882969661227967, recall=0.37690839694656486, fmeasure=0.04843951192593047)}\n",
      "Sumbasic in text 9\n",
      "{'rouge1': Score(precision=0.02576923076923077, recall=0.06393129770992366, fmeasure=0.03673245614035087), 'rougeL': Score(precision=0.02576923076923077, recall=0.06393129770992366, fmeasure=0.03673245614035087)}\n",
      "Textrank in text 9\n",
      "{'rouge1': Score(precision=0.05006603433785568, recall=0.3979007633587786, fmeasure=0.08894102591447159), 'rougeL': Score(precision=0.027374234601993036, recall=0.21755725190839695, fmeasure=0.04862962567985497)}\n"
     ]
    }
   ],
   "source": [
    "rogue1_precision_textrank = []\n",
    "rogueL_precision_textrank = []\n",
    "rogue1_recall_textrank = []\n",
    "rogueL_recall_textrank = []\n",
    "rogue1_precision_sumbasic = []\n",
    "rogueL_precision_sumbasic = []\n",
    "rogueL_recall_sumbasic = []\n",
    "rogue1_recall_sumbasic = []\n",
    "rogue1_precision_luhn = []\n",
    "rogueL_precision_luhn = []\n",
    "rogue1_recall_luhn = []\n",
    "rogueL_recall_luhn = []\n",
    "for i in range(10):\n",
    "    original = original_list[i]\n",
    "    test_summary = test_summary_list[i]\n",
    "    length = length_list[i]\n",
    "    summary_luhn = luhn(original, length)\n",
    "    summary_sumbasic = sumbasic(original, length)\n",
    "    summary_textrank = textrank(original, length)\n",
    "    scores_luhn = scorer.score(test_summary, summary_luhn)\n",
    "    print(f'Luhn in text {i}')\n",
    "    print(scores_luhn)\n",
    "    scores_sumbasic = scorer.score(test_summary, summary_sumbasic)\n",
    "    print(f'Sumbasic in text {i}')\n",
    "    print(scores_sumbasic)\n",
    "    scores_textrank = scorer.score(test_summary, summary_textrank)\n",
    "    print(f'Textrank in text {i}')\n",
    "    print(scores_textrank)\n",
    "    rogue1_precision_textrank.append(scores_textrank['rouge1'].precision)\n",
    "    rogueL_precision_textrank.append(scores_textrank['rougeL'].precision)\n",
    "    rogue1_recall_textrank.append(scores_textrank['rouge1'].recall)\n",
    "    rogueL_recall_textrank.append(scores_textrank['rougeL'].recall)\n",
    "    rogue1_precision_sumbasic.append(scores_sumbasic['rouge1'].precision)\n",
    "    rogueL_precision_sumbasic.append(scores_sumbasic['rougeL'].precision)\n",
    "    rogue1_recall_sumbasic.append(scores_sumbasic['rouge1'].recall)\n",
    "    rogueL_recall_sumbasic.append(scores_sumbasic['rougeL'].recall)\n",
    "    rogue1_precision_luhn.append(scores_luhn['rouge1'].precision)\n",
    "    rogueL_precision_luhn.append(scores_luhn['rougeL'].precision)\n",
    "    rogue1_recall_luhn.append(scores_luhn['rouge1'].recall)\n",
    "    rogueL_recall_luhn.append(scores_luhn['rougeL'].recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
