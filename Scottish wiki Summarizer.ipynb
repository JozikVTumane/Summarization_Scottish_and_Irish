{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItogSJcltxBa"
   },
   "source": [
    "# ÐŸÐ¸ÑˆÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¹Ð·ÐµÑ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJTEo-0dtxBd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "punctuation = punctuation + '\\n'\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOklQtHftxBe"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train([\"ga_dataset.txt\"], min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB-uihBclSgt"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train([\"gd.txt\"], min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J0hd3oLlJYH",
    "outputId": "2afdc058-0b7f-4d52-c209-06db24c62921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜gdwikimodelâ€™: File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gdwikimodel/vocab.json', 'gdwikimodel/merges.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir gdwikimodel\n",
    "tokenizer.save_model(\"gdwikimodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rb3nchtctxBg"
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./gdwikimodel/vocab.json\",\n",
    "    \"./gdwikimodel/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixN4t1bCtxBh"
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c81xvkZ8txBj"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQc_Iw0ZtxBk"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMQImDhttxBk"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./gdwikimodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn8eNYF5txBm"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pDkvfr4txBm",
    "outputId": "c8d895aa-929c-4600-c577-86b7af985e75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 1.23 s, total: 17.5 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"gd.txt\",\n",
    "    block_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lycj0mT0txBn"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-lswe9XtxBn",
    "outputId": "a0859a0a-c977-4884-e3d3-346badd6f197"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1453: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gdwikimodel\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=True,\n",
    "    no_cuda=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itcwoR8wtxBo"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "hNDiRdoRtxBo",
    "outputId": "5cbc16e7-415c-492b-b7d8-663f5bfd6168"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2960' max='7386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2960/7386 3:52:27 < 5:47:49, 0.21 it/s, Epoch 0.40/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.383900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1860\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m                 if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxiKqJF3txBo"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./gdwikimodel\", max_len=512)\n",
    "model = RobertaForMaskedLM.from_pretrained('./gdwikimodel/checkpoint-2500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRzle1TotxBr",
    "outputId": "e018b91a-abb5-4730-b53e-3231c68d170c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94SOZR3DtxBt"
   },
   "source": [
    "# Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYQzZSkgtxBu"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIDhaz42txBu"
   },
   "outputs": [],
   "source": [
    "def clean_tr(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r'http\\S+',' ',sentence)\n",
    "  sentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "  sentence = sentence.split()\n",
    "  sentence = [lem.lemmatize(word) for word in sentence]\n",
    "  sentence = ' '.join(sentence)\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "6NoaCbtFtxBs"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "def textrank(text, num_sentences=3):\n",
    "    # Text into sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Text into words\n",
    "    prepared_sentences = [clean_tr(sentence) for sentence in sentences]\n",
    "    words = [sentence.split() for sentence in prepared_sentences]\n",
    "    words = sum(words, []) #flatten the list\n",
    "    # calculate word frequencies\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    # Assign scores to sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(prepared_sentences):\n",
    "        for word in sentence.split():\n",
    "            if word in fdist:\n",
    "                if i in sentence_scores:\n",
    "                    sentence_scores[i] += fdist[word]\n",
    "                else:\n",
    "                    sentence_scores[i] = fdist[word]\n",
    "\n",
    "    # Sort sentences by scores in descending order\n",
    "    sorted_sentences = sorted(sentence_scores, key=lambda x: sentence_scores[x], reverse=True)\n",
    "\n",
    "    # Select the top `num_sentences` sentences for the summary\n",
    "    summary_sentences = sorted(sorted_sentences[:num_sentences])\n",
    "\n",
    "    # Create the summary\n",
    "    summary = ' '.join([sentences[i] for i in summary_sentences])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://gd.wikipedia.org/wiki/B%C3%A0nrigh_Ealasaid_II'\n",
    "text = extract_text_from_url(url)\n",
    "summary = textrank(text, 5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"'S i Ealasaid II (Ealasaid Alexandra MÃ iri; breith 21 an Giblean, 1926â€“8 an t-Sultain, 2022) BÃ nrigh an RÃ¬oghachd Aonaichte agus iomadach dhÃ¹thcha eile mun cuairt an t-saoghal ris an canar na Riaghaltan Co-Fhlaitheas; mar eisimpleir Canada. Gu lÃ¨ir, bha i na monarc tarsainn air Antigua agus Barbuda, AstrÃ ilia, Na h-Eileanan Bhathama, Belize, Canada, Grenada, Iaimeuca, Sealainn Nuadh, Papua Gini-Nuadh, Naomh CrÃ¬stean agus Nibheis , Naomh LÃ¹isia, Naomh Bhionsant agus Eileanan Greanadach, Na h-Eileanan Sholaimh, Tuvalu is an RÃ¬oghachd Aonaichte, am Breatainn MÃ²r agus Ãˆirinn a Tuath. Chaidh a h-athair, SeÃ²ras VI, a chrÃ¹nadh an deidh do a bhrÃ thair, Eideard VIII, leigeil an cathair rÃ¬oghail seachad ann an 1936. Riaghail SeÃ²ras VI gu 1952 agus, an deidh do bhÃ sachadh (an 6mh den Gearran), chaidh a chrÃ¹n gu an nighean is sine aige; Ealasaid. Chaidh a crÃ¹nadh mar banrigh air 2 an t-Ã’gmhios 1953. 'S e an duine aice, a phÃ²s i ann an 1947, am Prionnsa Philip, DiÃ¹c DhÃ¹n Ãˆideann agus 's ann 'on a' GhrÃ¨ig a tha e. Tha ceithir chlann aca le TeÃ rlach III as sine. Tha dÃ  mhac eile aca, Anndra agus Ãˆideard, agus aon nighean, Anna. Chaochail i 8 an t-Sultain 2022 ann am Baile Mhorail. 'S i a' chiad leanabh a bh' aig Prionnsa Albert, DiÃ¹c York agus a bhean, Ealasaid. 'S e a h-athair an dÃ rna mac aig RÃ¬gh SeÃ²ras V agus a' Bhanrigh MÃ iri. Rugadh i aig 2.40 sa mhadainn (GMT) air 21 dhan Ghiblean 1926 aig taigh a seanair: 17 SrÃ id Bruton, Mayfair, Lunnainn. Bha aon phiuthar aice, Mairead, a bha ceithir bliadhna nas Ã²ige na i. Thachair Ealasaid ri Prionnsa Philip ann an 1934. PhÃ²s iad air 20 dhan t-Samhain 1947 ann an Westminster Abbey. Fhuair iad thairis air 2,500 prÃ¨asantan bho air feadh an t-saoghal. Rugadh TeÃ rlach air 14 dhan t-Samhain 1948, agus a Bhana-phrionnsa Anna ann an 1950.\"\n",
    "summary = textrank(text, 5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIDMDHAtxB2"
   },
   "source": [
    "# SumBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHM656zKtxB3"
   },
   "outputs": [],
   "source": [
    "def clean_sb(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r'http\\S+',' ',sentence)\n",
    "  sentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "  sentence = sentence.split()\n",
    "  sentence = [lem.lemmatize(word) for word in sentence]\n",
    "  sentence = ' '.join(sentence)\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kUzr8KOtxB3"
   },
   "outputs": [],
   "source": [
    "def init_probability(sentences):\n",
    "    probability_dict = {}\n",
    "    #words = '. '.join(sentences)\n",
    "    words = [sentence.split() for sentence in sentences]\n",
    "    words = sum(words, []) #flatten the list\n",
    "    total_words = len(set(words))\n",
    "    for word in words:\n",
    "        if word!='.':\n",
    "            if not probability_dict.get(word):\n",
    "                probability_dict[word] = 1\n",
    "            else:\n",
    "                probability_dict[word] += 1\n",
    "\n",
    "    for word,count in probability_dict.items():\n",
    "        probability_dict[word] = count/total_words\n",
    "\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynMXCHBctxB3"
   },
   "outputs": [],
   "source": [
    "def update_probability(probability_dict,word):\n",
    "\tif probability_dict.get(word):\n",
    "\t\tprobability_dict[word] = probability_dict[word]**2\n",
    "\treturn probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8UGeS4otxB3"
   },
   "outputs": [],
   "source": [
    "def average_sentence_weights(sentences,probability_dict):\n",
    "\tsentence_weights = {}\n",
    "\tfor index,sentence in enumerate(sentences):\n",
    "\t\tif len(sentence) != 0:\n",
    "\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n",
    "\t\t\taverage_proba /= len(sentence)\n",
    "\t\t\tsentence_weights[index] = average_proba\n",
    "\treturn sentence_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mDRI8NhtxB4"
   },
   "outputs": [],
   "source": [
    "def generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n",
    "    summary = \"\"\n",
    "    current_length = 0\n",
    "    prev_sentence = []\n",
    "    while current_length < summary_length :\n",
    "\n",
    "        highest_probability_word = max(probability_dict)\n",
    "        #print(highest_probability_word)\n",
    "        sentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in sentence.split(' ')]\n",
    "        sentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n",
    "        #while ((sentence_list[0][0]) not in prev_sentence):\n",
    "        summary += cleaned_article[sentence_list[0][0]] + \". \"\n",
    "            #prev_sentence.append(sentence_list[0][0])\n",
    "            #sentence_list[0].pop(0)\n",
    "        for word in cleaned_article[sentence_list[0][0]]:\n",
    "            probability_dict = update_probability(probability_dict,word)\n",
    "        current_length+=1\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "oHPtTGsetxB4"
   },
   "outputs": [],
   "source": [
    "def sumbasic(article, required_length):\n",
    "    cleaned_article = []\n",
    "    for i in article.split('.'):\n",
    "        cleaned_article.append(clean_sb(i))\n",
    "    tokenized_article = tokenizer.encode(article)\n",
    "    #cleaned_article = clean(tokenized_article)\n",
    "    probability_dict = init_probability(cleaned_article)\n",
    "    #print(probability_dict.get('b'))\n",
    "    sentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n",
    "    summary = generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,required_length)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfhX9mQVtxB6"
   },
   "source": [
    "# luhn sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSUoTJY0juBh"
   },
   "outputs": [],
   "source": [
    "def clean_lh(article):\n",
    "\tlem = WordNetLemmatizer()\n",
    "\tarticle =  re.sub(r'\\[[^\\]]*\\]','',article)\n",
    "\tarticle = article.split('.')\n",
    "\tcleaned_list=[]\n",
    "\tfor sent in article:\n",
    "\t\tsent  = sent.lower()\n",
    "\t\tword_list = []\n",
    "\t\twords = sent.split()\n",
    "\t\tfor word in words:\n",
    "\t\t\tword_list.append(lem.lemmatize(word.lower()))\n",
    "\t\tcleaned_list.append(' '.join(word_list))\n",
    "\treturn cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmOROqXqtxB7"
   },
   "outputs": [],
   "source": [
    "def get_frequency_dictionary(content):\n",
    "\tfrequency = {}\n",
    "\tfor sentence in content:\n",
    "\t\tword_list = sentence.split()#word_tokenize(sentence)\n",
    "\t\tfor word in word_list:\n",
    "\t\t\tif word not in [',','.',';','%',')','(','``']:\n",
    "\t\t\t\tif frequency.get(word) is None:\n",
    "\t\t\t\t\tfrequency[word]=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfrequency[word]+=1\n",
    "\treturn frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNmDLdaKj6bc"
   },
   "outputs": [],
   "source": [
    "def get_score(content,frequency_dictionary):\n",
    "    sentence_score={}\n",
    "    for sentence in content:\n",
    "        score=0\n",
    "        word_list = sentence.split()\n",
    "        start_idx,end_idx = -1,len(word_list)+1\n",
    "        index_list=[]\n",
    "        for word in word_list:\n",
    "            if word not in [',','.',';','%',')','(','``'] and word in frequency_dictionary.keys():\n",
    "                index_list.append(word_list.index(word)+1)\n",
    "            if index_list:\n",
    "                if max(index_list) != min(index_list):\n",
    "                    score = len(index_list)**2/(max(index_list) - min(index_list))\n",
    "                else:\n",
    "                    score = len(index_list)**2/max(index_list)\n",
    "        sentence_score[content.index(sentence)] = score\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XXMyU3EtxB7"
   },
   "outputs": [],
   "source": [
    "def get_summary_luhn(sentence_scores,content,threshold):\n",
    "    summary = \"\"\n",
    "    sentence_indexes = sorted(sentence_scores,key=sentence_scores.get,reverse=True)[:threshold-1]\n",
    "    for index in sentence_indexes:\n",
    "        summary+=content[index]+\" \"\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "KaUNY0vmtxB8"
   },
   "outputs": [],
   "source": [
    "def luhn(content, word_limit):\n",
    "    cleaned_content = clean_lh(content)\n",
    "    threshold = len(cleaned_content)//40\n",
    "    frequency_dictionary = get_frequency_dictionary(cleaned_content)\n",
    "    sorted_dictionary = {key: frequency_dictionary[key] for key in sorted(frequency_dictionary,key=frequency_dictionary.get,reverse=True)[:word_limit]}\n",
    "    sentence_scores = get_score(cleaned_content,sorted_dictionary)\n",
    "    summary = get_summary_luhn(sentence_scores,cleaned_content,threshold)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDcIh73UkByC"
   },
   "source": [
    "# Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "F7l2zWNPkEB8"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "oD6Mb4l7fWhu"
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "original_list = []\n",
    "test_summary_list = []\n",
    "length_list = []\n",
    "with bz2.open('gd_test.tar.bz2', 'rt', encoding='UTF-8') as f:\n",
    "  for i in range(10):\n",
    "    a = f.readline()\n",
    "    b = a[a.find('{'):]\n",
    "    c = json.loads(b)\n",
    "    original_list.append(c['text'])\n",
    "    test_summary_list.append(c['summary'])\n",
    "    length_list.append(len(c['summary'].split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC9pkVnCu1AJ",
    "outputId": "95521a37-f844-4283-ae37-3022ec27018b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn in text 0\n",
      "{'rouge1': Score(precision=0.06936416184971098, recall=0.5217391304347826, fmeasure=0.12244897959183672), 'rougeL': Score(precision=0.05202312138728324, recall=0.391304347826087, fmeasure=0.09183673469387756)}\n",
      "Sumbasic in text 0\n",
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.17391304347826086, fmeasure=0.1951219512195122), 'rougeL': Score(precision=0.2222222222222222, recall=0.17391304347826086, fmeasure=0.1951219512195122)}\n",
      "Textrank in text 0\n",
      "{'rouge1': Score(precision=0.125, recall=0.391304347826087, fmeasure=0.18947368421052635), 'rougeL': Score(precision=0.06944444444444445, recall=0.21739130434782608, fmeasure=0.10526315789473684)}\n",
      "Luhn in text 1\n",
      "{'rouge1': Score(precision=0.09202453987730061, recall=0.5769230769230769, fmeasure=0.15873015873015872), 'rougeL': Score(precision=0.06748466257668712, recall=0.4230769230769231, fmeasure=0.1164021164021164)}\n",
      "Sumbasic in text 1\n",
      "{'rouge1': Score(precision=0.11290322580645161, recall=0.2692307692307692, fmeasure=0.1590909090909091), 'rougeL': Score(precision=0.0967741935483871, recall=0.23076923076923078, fmeasure=0.13636363636363635)}\n",
      "Textrank in text 1\n",
      "{'rouge1': Score(precision=0.12987012987012986, recall=0.38461538461538464, fmeasure=0.19417475728155337), 'rougeL': Score(precision=0.1038961038961039, recall=0.3076923076923077, fmeasure=0.15533980582524273)}\n",
      "Luhn in text 2\n",
      "{'rouge1': Score(precision=0.12, recall=0.75, fmeasure=0.20689655172413793), 'rougeL': Score(precision=0.09333333333333334, recall=0.5833333333333334, fmeasure=0.16091954022988506)}\n",
      "Sumbasic in text 2\n",
      "{'rouge1': Score(precision=0.2727272727272727, recall=0.5, fmeasure=0.3529411764705882), 'rougeL': Score(precision=0.2727272727272727, recall=0.5, fmeasure=0.3529411764705882)}\n",
      "Textrank in text 2\n",
      "{'rouge1': Score(precision=0.24096385542168675, recall=0.5555555555555556, fmeasure=0.3361344537815126), 'rougeL': Score(precision=0.1686746987951807, recall=0.3888888888888889, fmeasure=0.2352941176470588)}\n",
      "Luhn in text 3\n",
      "{'rouge1': Score(precision=0.030470914127423823, recall=0.9166666666666666, fmeasure=0.058981233243967826), 'rougeL': Score(precision=0.024930747922437674, recall=0.75, fmeasure=0.0482573726541555)}\n",
      "Sumbasic in text 3\n",
      "{'rouge1': Score(precision=0.075, recall=0.25, fmeasure=0.11538461538461538), 'rougeL': Score(precision=0.075, recall=0.25, fmeasure=0.11538461538461538)}\n",
      "Textrank in text 3\n",
      "{'rouge1': Score(precision=0.12857142857142856, recall=0.75, fmeasure=0.2195121951219512), 'rougeL': Score(precision=0.1, recall=0.5833333333333334, fmeasure=0.17073170731707318)}\n",
      "Luhn in text 4\n",
      "{'rouge1': Score(precision=0.062146892655367235, recall=0.88, fmeasure=0.11609498680738788), 'rougeL': Score(precision=0.04519774011299435, recall=0.64, fmeasure=0.08443271767810026)}\n",
      "Sumbasic in text 4\n",
      "{'rouge1': Score(precision=0.02631578947368421, recall=0.04, fmeasure=0.031746031746031744), 'rougeL': Score(precision=0.02631578947368421, recall=0.04, fmeasure=0.031746031746031744)}\n",
      "Textrank in text 4\n",
      "{'rouge1': Score(precision=0.1095890410958904, recall=0.32, fmeasure=0.16326530612244897), 'rougeL': Score(precision=0.0958904109589041, recall=0.28, fmeasure=0.14285714285714285)}\n",
      "Luhn in text 5\n",
      "{'rouge1': Score(precision=0.04776119402985075, recall=0.5517241379310345, fmeasure=0.0879120879120879), 'rougeL': Score(precision=0.029850746268656716, recall=0.3448275862068966, fmeasure=0.054945054945054944)}\n",
      "Sumbasic in text 5\n",
      "{'rouge1': Score(precision=0.08333333333333333, recall=0.10344827586206896, fmeasure=0.09230769230769231), 'rougeL': Score(precision=0.08333333333333333, recall=0.10344827586206896, fmeasure=0.09230769230769231)}\n",
      "Textrank in text 5\n",
      "{'rouge1': Score(precision=0.13580246913580246, recall=0.3793103448275862, fmeasure=0.19999999999999998), 'rougeL': Score(precision=0.07407407407407407, recall=0.20689655172413793, fmeasure=0.1090909090909091)}\n",
      "Luhn in text 6\n",
      "{'rouge1': Score(precision=0.017985611510791366, recall=0.9090909090909091, fmeasure=0.03527336860670194), 'rougeL': Score(precision=0.01618705035971223, recall=0.8181818181818182, fmeasure=0.031746031746031744)}\n",
      "Sumbasic in text 6\n",
      "{'rouge1': Score(precision=0.08, recall=0.18181818181818182, fmeasure=0.1111111111111111), 'rougeL': Score(precision=0.08, recall=0.18181818181818182, fmeasure=0.1111111111111111)}\n",
      "Textrank in text 6\n",
      "{'rouge1': Score(precision=0.08585858585858586, recall=0.7727272727272727, fmeasure=0.15454545454545454), 'rougeL': Score(precision=0.05555555555555555, recall=0.5, fmeasure=0.09999999999999999)}\n",
      "Luhn in text 7\n",
      "{'rouge1': Score(precision=0.060109289617486336, recall=0.6470588235294118, fmeasure=0.11), 'rougeL': Score(precision=0.0546448087431694, recall=0.5882352941176471, fmeasure=0.10000000000000002)}\n",
      "Sumbasic in text 7\n",
      "{'rouge1': Score(precision=0.037037037037037035, recall=0.11764705882352941, fmeasure=0.05633802816901408), 'rougeL': Score(precision=0.037037037037037035, recall=0.11764705882352941, fmeasure=0.05633802816901408)}\n",
      "Textrank in text 7\n",
      "{'rouge1': Score(precision=0.0625, recall=0.29411764705882354, fmeasure=0.10309278350515463), 'rougeL': Score(precision=0.05, recall=0.23529411764705882, fmeasure=0.08247422680412371)}\n",
      "Luhn in text 8\n",
      "{'rouge1': Score(precision=0.06837606837606838, recall=0.6666666666666666, fmeasure=0.124031007751938), 'rougeL': Score(precision=0.042735042735042736, recall=0.4166666666666667, fmeasure=0.07751937984496124)}\n",
      "Sumbasic in text 8\n",
      "{'rouge1': Score(precision=0.20588235294117646, recall=0.2916666666666667, fmeasure=0.2413793103448276), 'rougeL': Score(precision=0.14705882352941177, recall=0.20833333333333334, fmeasure=0.1724137931034483)}\n",
      "Textrank in text 8\n",
      "{'rouge1': Score(precision=0.08108108108108109, recall=0.25, fmeasure=0.12244897959183675), 'rougeL': Score(precision=0.05405405405405406, recall=0.16666666666666666, fmeasure=0.0816326530612245)}\n",
      "Luhn in text 9\n",
      "{'rouge1': Score(precision=0.0880503144654088, recall=0.5, fmeasure=0.1497326203208556), 'rougeL': Score(precision=0.06289308176100629, recall=0.35714285714285715, fmeasure=0.10695187165775401)}\n",
      "Sumbasic in text 9\n",
      "{'rouge1': Score(precision=0.03225806451612903, recall=0.07142857142857142, fmeasure=0.044444444444444446), 'rougeL': Score(precision=0.03225806451612903, recall=0.07142857142857142, fmeasure=0.044444444444444446)}\n",
      "Textrank in text 9\n",
      "{'rouge1': Score(precision=0.2054794520547945, recall=0.5357142857142857, fmeasure=0.297029702970297), 'rougeL': Score(precision=0.1232876712328767, recall=0.32142857142857145, fmeasure=0.1782178217821782)}\n"
     ]
    }
   ],
   "source": [
    "rogue1_precision_textrank = []\n",
    "rogueL_precision_textrank = []\n",
    "rogue1_recall_textrank = []\n",
    "rogueL_recall_textrank = []\n",
    "rogue1_precision_sumbasic = []\n",
    "rogueL_precision_sumbasic = []\n",
    "rogueL_recall_sumbasic = []\n",
    "rogue1_recall_sumbasic = []\n",
    "rogue1_precision_luhn = []\n",
    "rogueL_precision_luhn = []\n",
    "rogue1_recall_luhn = []\n",
    "rogueL_recall_luhn = []\n",
    "for i in range(10):\n",
    "    original = original_list[i]\n",
    "    test_summary = test_summary_list[i]\n",
    "    length = length_list[i]\n",
    "    summary_luhn = luhn(original, length)\n",
    "    summary_sumbasic = sumbasic(original, length)\n",
    "    summary_textrank = textrank(original, length)\n",
    "    scores_luhn = scorer.score(test_summary, summary_luhn)\n",
    "    print(f'Luhn in text {i}')\n",
    "    print(scores_luhn)\n",
    "    scores_sumbasic = scorer.score(test_summary, summary_sumbasic)\n",
    "    print(f'Sumbasic in text {i}')\n",
    "    print(scores_sumbasic)\n",
    "    scores_textrank = scorer.score(test_summary, summary_textrank)\n",
    "    print(f'Textrank in text {i}')\n",
    "    print(scores_textrank)\n",
    "    rogue1_precision_textrank.append(scores_textrank['rouge1'].precision)\n",
    "    rogueL_precision_textrank.append(scores_textrank['rougeL'].precision)\n",
    "    rogue1_recall_textrank.append(scores_textrank['rouge1'].recall)\n",
    "    rogueL_recall_textrank.append(scores_textrank['rougeL'].recall)\n",
    "    rogue1_precision_sumbasic.append(scores_sumbasic['rouge1'].precision)\n",
    "    rogueL_precision_sumbasic.append(scores_sumbasic['rougeL'].precision)\n",
    "    rogue1_recall_sumbasic.append(scores_sumbasic['rouge1'].recall)\n",
    "    rogueL_recall_sumbasic.append(scores_sumbasic['rougeL'].recall)\n",
    "    rogue1_precision_luhn.append(scores_luhn['rouge1'].precision)\n",
    "    rogueL_precision_luhn.append(scores_luhn['rougeL'].precision)\n",
    "    rogue1_recall_luhn.append(scores_luhn['rouge1'].recall)\n",
    "    rogueL_recall_luhn.append(scores_luhn['rougeL'].recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf46yTNLsEzz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
